<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <title>How Computers Think: A Resume Job Field Classification Model in Python / Kevin Price</title>
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width">

        <link rel="shortcut icon" href="./theme/favicon.ico" />
        <link rel="apple-touch-icon" href="./theme/apple-touch-icon-iphone.png" />
        <link rel="apple-touch-icon" sizes="72x72" href="./theme/apple-touch-icon-ipad.png" />
        <link rel="apple-touch-icon" sizes="114x114" href="./theme/apple-touch-icon-iphone4.png" />
        <link rel="apple-touch-icon" sizes="144x144" href="./theme/apple-touch-icon-ipad3.png" />

        <link rel="stylesheet" href="./theme/css/bootstrap.min.css">
        <link rel="stylesheet" href="./theme/css/bootstrap-responsive.min.css">
        <link rel="stylesheet" href="./theme/css/font-awesome.min.css">
        <link rel="stylesheet" href="./theme/css/colorbox.css">

        <link rel="stylesheet" href="./theme/css/main.css">

        <script src="./theme/js/vendor/modernizr-2.6.2-respond-1.1.0.min.js"></script>
    </head>
    <body>

        <div class="container">
            <div class="row">
                <div class="span9">
                    <div class="row">
                        <div class="span2"> <!-- logo -->
                            <div id="logo">
                                <a href=".">
                                    <img alt="Kevin Price" src=".//images/me.jpg" />
                                </a>
                            </div> <!-- logo -->
                        </div> <!-- logo span -->
						
                        <div class="span7"> <!-- header -->
                            <!--[if lt IE 9]>
                                <p class="chromeframe alert alert-warning">You are using an <strong>outdated</strong> browser, and this site might not look best in it. Please <a href="http://browsehappy.com/">upgrade your browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">activate Google Chrome Frame</a> to improve your experience.</p>
                            <![endif]-->
                            <header id="site-header">
                                <div id="site-header-content">
                                    <h1>
                                        <a href=".">Kevin Price</a>
                                        <small>
                                            <span class="divider">/</span> 
                                        </small>
                                    </h1>
                                    <p>
                                        I am a data science intern and computer programmer. Here are some of my projects and ideas.
                                    </p>
                                </div>
                            </header>
                        </div> <!-- header span -->
                    </div> <!-- header row -->

                    <div class="row"> <!-- content -->
                        <div class="content">
<div class="article-page">
    <div class="span7 offset2">
        <header class="article">
            <h2><a href="./resume-model.html" rel="bookmark">How Computers Think: A Resume Job Field Classification Model in Python</a></h2>
			<div style="display:inline-block; margin-top: 3px; font-family: Baskerville, Garamond, Palatino;">— Computer Modeling —&nbsp;&nbsp;</div>
            <time datetime="2017-05-13T00:00:00-04:00">
                May 13, 2017
            </time>
        </header>
    </div>

    <div class="span2">
    <div class="article-meta article-meta-border">
        <dl class="dl-horizontal">
            <dt><i class="icon-file"></i></dt>
            <dd>
                <a href="./category/computer-modeling.html">
                    Computer Modeling
                </a>
            </dd>

            <dt><i class="icon-tags"></i></dt>
            <dd>
                    <a href="./tag/machine-learning.html">Machine Learning</a>
<span class="separator">/</span>                    <a href="./tag/python.html">Python</a>
            </dd>
        </dl>

        <b class="border-notch notch"></b>
        <b class="notch"></b>
    </div>
    </div>

    <div class="span7">
        <section>
            <article class="entry">
    <!-- Summary: A Python model to classify resumes based on their job type. -->

<p>Can a computer correctly classify your resume into its correct job field? I recently built a classification model for my work, Mosaic Data Science, that answers this question. Depending on how you look at it, this model is either 89% or 82% accurate.</p>
<p>My boss granted me permission to post an article about this project to my website. I have decided to use this project to teach you how a computer “thinks.” I will attempt to explain myself as clearly as possible so that many readers can understand the data science concepts I have been working with. I hope that this article may enlighten you about how machine learning works.</p>


<p><img style="float: right; margin-left:10px; margin-top:-5px;" src="images/fixing-unplugged-computer-clipart_750-754.jpeg" width=275 height=275 alt="https://clipartfest.com/download/bb6ad6c6ec516c0883d3f33d885b02cca169f21b.html"/></p>
<p>This project was written as a demonstration for DARPA, the Defense Advanced Research Projects Agency. DARPA has another project in mind for Mosaic, contingent on our demonstration of a resume classification model. Thus I was tasked with building this model, with guidance from other employees at the company. The other employees spent a good deal of time advising me, though I wrote the vast majority of the code. I ultimately ended up abandoning other approaches to the model and creating our final approach to the model myself, resulting in a model with better performance. I am nonetheless very grateful for the valuable data science advice I continued to receive from the employees around me as I built this model.</p>
<h3>The Model</h3>
<p>After a great deal of experimenting with the advice of my colleagues, I discovered a code sample on scikit-learn’s website that does precisely what I am looking for. This code classifies news articles into 20 unique newsgroups, which is a very similar problem to classifying resumes by job-field.</p>
<p>After experimenting with many other model versions, I ultimately adopted this code and the score improved dramatically. This code source is found here:</p>
<p><a href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html">http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html</a></p>
<p>After experimenting with the available classifiers from scikit-learn, I determined that the best model type for this feature setup was a Random Forest Classifier.</p>
<p>A Random Forest is a collection of Decision Trees; and now, of course, I need to explain decision trees! A decision tree as a resume classifier would take the resumes and make a web of statistical choices about each resume. It would attempt to find which words are most strongly correlated with each topic and then use this information to make a category decision.</p>
<p>To explain decision trees, I will reference an example from the Kaggle website. I have spent a good deal of time studying their dataset from the Titanic disaster. Kaggle is hosting a machine-learning competition to generate a model that can predict the survival of Titanic passengers. For this model, the most obvious decision tree feature would be whether the passenger is male or female; females had a much higher survivability rate than males. Next, I could gauge which class the passenger was in. Third class had poor survivability, while first class was much more likely to escape alive. These features would be different nodes on a decision tree. There is also an element of randomness in the decision tree; thus, it will not necessarily produce exactly the same result every time. </p>
<p>For the resume classification project, a decision tree would map out correlations between words. The computer generates a forest of different decision trees and averages out their answers to get the best category choice for each resume. This is a Random Forest.</p>
<h3>Data Source</h3>
<p><img style="float: left; margin-right:10px;" src="images/k19403833.jpg" width=200 height=200 alt="Resume clip-art" title="http://www.fotosearch.com/CSP538/k19403833/"/></p>
<p>In order to build a resume classifier, I need resumes to classify! Fortunately, another intern at the company was tasked with the job of downloading batches of resumes from Indeed.com using shell scripts which were written by my supervisor. This intern downloaded 1000 resumes for each of 100 different job fields, in batches of 50. His job must have been very tedious.</p>
<p>As I combed through the resume data, my boss and I discovered a problem: the downloaded resumes did not always match the supposed job field they represented! As I was showing my boss the input files, he found a resume that appeared completely irrelevant. This surprised me, and prompted me to investigate further. My colleagues and I had previously assumed this data was “correct.” In other words, we thought what we were downloading actually represented the field it was supposed to represent. Unfortunately, my boss and I only discovered this problem after the temporary employee who downloaded the resumes had already left. A computer model will not work if it is trained on faulty data!</p>
<p>At this point, the model already appeared to be working marvellously! The model produced a high score, but obviously the underlying code was producing faulty predictions, since the data was incorrect. This meant the model was perpetuating the false job-fields it was being fed in. Something was similar in the returned resume results, or the model would not be able to classify the resumes at all. The computer model was successfully finding trends in the faulty resume data.</p>
<p>The downloads appeared to work like a Google search, with the most relevant search results at the top of the list, and less relevant results later. As you went down the results list, searches for “aircraft mechanic” might start turning up auto mechanics, for example. Then, completely irrelevant results would start turning up. Something in these resumes must have matched the original search used to download the resumes; the model was successfully classifying them.</p>
<p>This problem was more obvious with some fields than with others; for instance, there were almost no “helicopter pilot” resumes available at all, but there were hundreds of dental hygienists. This trend probably depends on which job-fields are most searched for on Indeed.com.</p>
<p>To fix this problem, I created an algorithm to verify the resume results. It matched each resume “headline”—a metadata field that was not displayed on the actual resume—to the job field it was supposed to represent. If any of the words in the job field were not in the headline, then that resume was thrown out. Furthermore, if my purging strategy left me with less than 100 resumes, then I threw out that job field altogether, dismissing it as low-quality. This appeared to fix the problem. It did not significantly change the model’s score, but the returned resumes finally appeared to match the fields they <em>should</em> represent.</p>
<h3>Natural Language Processing</h3>
<p>Building this computer model requires language processing. The computer must be able, at some level, to understand the text that it is being fed.</p>
<p>The computer needed to process, simplify, and standardize the text in each resume. It needed to mathematically rate how important each word is to its particular job-field. It had to generate a standard list of features that the model could look at and compare.</p>
<p>Unfortunately, computers are not advanced enough to read text and “understand” what this text means. Companies have worked at this problem for years and have generated some very clever scripts. Some algorithms are able to extract a good deal of meaning from text, but no computer is able to simply “read” a random document and explain to you “in its own words” what it means. There are some highly complex algorithms out there to extract limited meaning from text, but such approaches are much more expensive and less common. A human can use English grammar to read a resume; a computer looks at text through statistics and mathematics.</p>
<p>Natural Language Processing is an emerging field that uses computers to extract meaning from text. Most of the work in this field uses a “bag of words” theory to study documents. In other words, the computer looks at a document statistically as if it were simply a pile of words, and it attempts to figure out which words are relevant to the topic. </p>
<p><img style="float: right; margin-left:10px; margin-bottom:10px;" src="images/1252997748713475.jpeg" width=350 height=350 alt="scrabble clip-art" title="http://www.okclipart.com/Scrabble-Players-Clip-Art30khensaze/"/></p>
<p>In order for my “bag of words” to be useful to a computer, I needed to standardize the text of each resume. I threw out all extraneous characters such as bullets and punctuation, and kept only the characters that follow the ASCII format. In other words, I kept only standard characters and eliminated any foreign characters or symbols. Then, I lemmatized everything. Lemmas allow the model to recognize many forms of a word as one word. Type, typing, and typed each share the same lemma (or root) of “type.” Thus, lemmatizing would standardize the three word forms to just “type.” Three separate words would only confuse the model; the model should be able to recognize these as one word. Lemmatizing the text additionally removes all capitalization.</p>
<p>Next, the computer must use mathematics to determine which words are important to each resume. The process we used is called TF-IDF, or Term Frequency-Inverse Document Frequency. Fortunately, scikit-learn provides a tool to do this automatically.</p>
<p>A word that is important to a document should occur frequently in that document and infrequently in the others. TF-IDF capitalizes on this by counting up all of the words in each of the resumes and comparing these counts to each other. Each word in every resume gets a TF-IDF “score” which tells the computer how important the word theoretically is to that resume. All of these TF-IDF scores were placed in a massive table and compared to each other. This table contains all of the possible words in the entire dataset as columns (millions of unique words), and all of the different resumes as rows. If a word does not exist in a resume, then it simply gets a score of 0.</p>
<p>TF-IDF rates how important each word is to a particular resume, but it does not rate how important the word is to the job field! To accomplish this, I discovered another handy feature in the scikit-learn package: a chi^2 importance test. Essentially, the script uses this test to determine which words in my large TF-IDF table were most correlated to each job field. I used this tool to determine which resume words to keep and which to discard. This left me with a small overall subset of highly relevant words. I determined that the overall top 1000 most-relevant words was the best selection.</p>
<p>Here is some of the code from the text processing function:</p>
<div class="highlight"><pre><span></span>def process(fields, allkeys, docID, map_output, res_output, jobwords):
    &quot;&quot;&quot;Takes each resume in .json format and processes it&quot;&quot;&quot;

    #remove duplicate keys before processing them. I want unique resumes.
    key = fields[&#39;accountKey&#39;] #get the document key
    if key in allkeys:
        return

    #Now do a quality check, to make sure the resume actually is what we think it is.
    if not check_jobwords( fields, jobwords ):
        return

    string = extract(fields)    #Extract resume fields into a string
    string = remove_garbage(string)   #Remove garbage characters

    #skip resume if it&#39;s less than 10 words. Don&#39;t want it.
    if len(string.split()) &lt;= 10:
        return

    string = lemmatize(string) #lemmatize

    #Write the results to the disk
    res_output.write( str(docID) + &quot; &quot; + string + &#39;\n&#39; ) #&quot;Processed_resumes.txt&quot;
    map_output.write( str(docID) + &#39;,&#39; + key + &#39;\n&#39;) #docID : document-key mapping

    return key   #this is necessary to prevent duplicate resumes.
</pre></div>


<h3>Principal Component Analysis</h3>
<p>One of the requirements of the project was that it include Principal Component Analysis. PCA is a method that can reduce the number of features used by the model while losing as little data as possible.</p>
<p>Here is an example of how PCA works. Say a group of points are correlated and produce a nearly straight line. You could simplify the data by flattening it out, and still capture the majority of the variation in the data. Essentially, you can “boil down” the data to something simpler, while still capturing most of what makes the data unique. This process is called “dimensionality reduction.”</p>
<p><img style="float: center; margin-bottom:10px;" src="images/PCA.png" width=600 height=200 alt="PCA demonstration" title="http://setosa.io/ev/principal-component-analysis/"/></p>
<p>What the Random Forest model “sees” is a huge table. Each of the possible resume words are separate columns, and each of the resumes is a different row. The different columns in my data table are called “features.” After processing them through SVD, they become “components.” This table is 1000 columns (words)  x ~80,000 rows (resumes) large. The table does not store the actual words; it saves the TF-IDF scores from each word.</p>
<p>Theoretically, less data input would mean less “overfitting.” In other words, reducing the data would streamline the model and prevent it from extrapolating excessive information or seeing trends where there are none. Unfortunately, using PCA brought the score down slightly instead of improving it, hence the model is 82% accurate instead of 89%. In my case, PCA was less helpful than expected.</p>
<p>For my model, I used a method that is very similar to PCA called Singular Value Decomposition (SVD), because the data input is sparse. PCA cannot work on sparse inputs, yet SVD can. Nonetheless, the general process used is the same.</p>
<p>Sparse data is another way of using less computer memory to store information. If most of the datapoints are zero, then instead of saving all of the “zeros,” we can save only the datapoints that actually have data. This is the case for most of the words in my table, since most of the possible words will not be represented in a given resume.</p>
<p><img style="float: right; margin-left:10px; margin-bottom:10px;" src="images/ExplainedVariance.png" width=415 height=415 alt="graph of variance from each new component"/></p>
<p>I graphed the amount of information gained by each new SVD component, and I determined that 200 was about the optimal number of components (graph on right). After 200 components (boiled-down words), the model does not gain “new” information as quickly from each new component.</p>
<p>I can use PCA to reduce 1000 word-features into 200! PCA will find correlations among the words and find the simplest way to represent this data. The first 200 PCA components will show the majority of variation from the 1000 words. Thus, I can minimize the data stream that is fed into the model by incorporating fewer features.</p>
<h3>Other Tests</h3>
<p>I experimented with many other features as I refined the model, with the continued help of my colleagues. I tried using different packages for the features I needed. For instance, I tried using Gensim’s TF-IDF algorithm instead of scikit-learn’s. I discovered that scikit-learn’s algorithm is much faster and more effective for computer modeling.</p>
<p>I tested the different model types found in the 20-newsgroups sample on scikit-learn’s website; the Random Forest model scored the best.</p>
<p>Another version of my random forest model used bigrams instead of raw resume words. In other words, I matched all of the words in pairs of two in an attempt to capture connections between words. I then used TF-IDF to capture only those bigrams that contain words that are in the top 10% of relevance. This model was dramatically worse; it produced a much lower score.</p>
<h3>Scoring</h3>
<p>There are several different metrics to use to score a classification model. Each score provides a different window into how the model is performing. I used accuracy, precision, recall, and fscore. The computer randomly selects a segment of the resumes to <em>train</em> the model and another segment to <em>test</em> its predictive power and produce a score.</p>
<p>Accuracy simply looks at the model and says, “Of all of the answers, how many did I get right?” Precision asks, “Of all of the answers that were labeled in category X, how many actually are in category X?” And then recall asks, “Of all of the answers that should be in category X, how many did you correctly mark in category X?” The fscore attempts to strike a balance between precision and recall.</p>
<p>Because of my resume validation exercise (purging invalid job fields), the number of resumes in each field was dramatically different. Some fields contained 100 resumes; some fields contained 800! This confused the model and produced a convoluted set of scores:</p>
<table border="1" cellspacing="8" cellpadding="8" style="font-family:Georgia; margin-left:10px;">
<tr>
    <td>Accuracy</td>
    <td>86.3%</td>
</tr>
<tr>
    <td>Precision</td>
    <td>84.2%</td>
</tr>
<tr>
    <td>Recall</td>
    <td>74.0%</td>
</tr>
<tr>
    <td>Fscore</td>
    <td>76.4%</td>
</tr>
</table>

<p><br/></p>
<p>To balance these scores, I discovered a class_weight=”balanced” feature hidden in the Random Forest code. This tells the model that the different job field categories are unevenly stacked. Simply adding this setting improved all of the scores:</p>
<table border="1" cellspacing="8" cellpadding="8" style="font-family:Georgia; margin-left:10px;">
<tr>
    <td>Accuracy</td>
    <td>89.2%</td>
</tr>
<tr>
    <td>Precision</td>
    <td>88.9%</td>
</tr>
<tr>
    <td>Recall</td>
    <td>85.4%</td>
</tr>
<tr>
    <td>Fscore</td>
    <td>86.3%</td>
</tr>
</table>

<p><br/></p>
<p>Adding in the SVD (PCA) reduced the score slightly and resulted in this final output:</p>
<p><img style="float: center; margin-bottom:10px;" src="images/output.png" alt="final output"/></p>
<h3>Conclusion</h3>
<p>The computer can successfully identify a job field from a resume. This is especially surprising considering that many people change job fields; thus, their resumes may contain aspects of both fields.</p>
<p>In a model with ~90 job fields and thousands of resumes, an 82% accuracy score seems very impressive. If the number of included job fields were increased dramatically, it is likely that the model would become confused between similar fields, such as “aircraft mechanic” and “auto mechanic.” However, at the current level of complexity, the model functions very well.</p>

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'priceisright204'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
		(function() { // DON'T EDIT BELOW THIS LINE
			var d = document, s = d.createElement('script');
			s.src = '//' + disqus_shortname + '.disqus.com/embed.js';
			s.setAttribute('data-timestamp', +new Date());
			(d.head || d.body).appendChild(s);
		})();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </article>
        </section>
    </div> <!-- span -->
</div>                        </div>
                    </div> <!-- content row -->

                    <div class="row"> <!-- footer -->
                        <div class="span7 offset2">
                            <footer id="site-footer">
                                <p>
                                    <a href="" target="_blank">
                                        
                                    </a> Kevin Price /
                                    <a href="./">Archives</a>
                                </p>
                                <p>
                                    Powered by
                                    <a href="https://github.com/getpelican/pelican" target="_blank">
                                        Pelican
                                    </a>
                                    /
                                    My code on
                                    <a href="http://github.com/Priceisright204" target="blank">
                                        GitHub
                                    </a>
                                    /
                                    Modified <code>Lannisport</code> theme
                                </p>
                            </footer>
                        </div> <!-- footer span -->
                    </div> <!-- footer row -->
                </div> <!-- header+content+footer span -->

                <div class="span3">
                    <aside class="affix">
                        <div id="sidebar">
                            <div id="social-icons">
                                    <span class="social-icon">
                                        <a href="https://github.com/priceisright204" target="_blank" title="GitHub">
                                            <i class="icon-github-alt"></i>
                                        </a>
                                    </span>
                                    <span class="social-icon">
                                        <a href="https://www.linkedin.com/in/kevin-price-371a9a6b" target="_blank" title="LinkedIn">
                                            <i class="icon-linkedin"></i>
                                        </a>
                                    </span>
                                    <span class="social-icon">
                                        <a href="http://facebook.com/kevin.price.204" target="_blank" title="Facebook">
                                            <i class="icon-facebook"></i>
                                        </a>
                                    </span>
                            </div>

                            <div id="navigation">
<nav>
	<h2>Pages</h2>
	<ul>
		<li>
			<ul>
				<li>
					<a href=".">Site Home</a>
				</li>

					</br><li><a href="https://thepricesareright.wordpress.com/" target="_blank">
Our family blog, written by my wife Erin
</a></li>
					</br><li><a href="./maze/instructions.html" target="_blank">Maze</a></li>

			</ul>
		</li>
	</ul>
</nav>

<nav>
	<h2>Categories</h2>
	<ul>
		<li>
			<ul>
					</br><li>
						<a href="./category/about-this-site.html">About this site</a>
					</li>
					</br><li>
						<a href="./category/computer-modeling.html">Computer Modeling</a>
					</li>
					</br><li>
						<a href="./category/games.html">Games</a>
					</li>
			</ul>
		</li>
	</ul>
</nav>                            </div>
                        </div> <!-- sidebar -->

                        <p id="back-to-top">
                            <a href="#">
                                <i class="icon-double-angle-up"></i> Back to top
                            </a>
                        </p>

                    </aside>
                </div> <!-- logo+navigation span -->
            </div> <!-- row -->

        </div> <!-- /container -->

        <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
        <script>window.jQuery || document.write('<script src="./theme/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>

        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.0/js/bootstrap.min.js"></script>
        <script>$('body').popover || document.write('<script src="./theme/js/vendor/bootstrap.min.js"><\/script>')</script>

        <script src="./theme/js/jquery.captions.js"></script>
        <script src="./theme/js/vendor/jquery.colorbox-min.js"></script>

        <script src="./theme/js/main.js"></script>

        <script>
            var _gaq=[['_setAccount',''],['_trackPageview'],['_trackPageLoadTime'],['_gat._anonymizeIp']];
            (function(d,t){var g=d.createElement(t),s=d.getElementsByTagName(t)[0];
            g.src=('https:'==location.protocol?'//ssl':'//www')+'.google-analytics.com/ga.js';
            s.parentNode.insertBefore(g,s)}(document,'script'));
        </script>
    </body>
</html>