<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-05-23T21:27:30-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Kevin Price</title><subtitle>I am a computer programmer and math instructor. This is the home for my personal projects and activities.</subtitle><author><name>Kevin Price</name></author><entry><title type="html">How Humanity Discovered the Area of a Circle</title><link href="http://localhost:4000/mathematics/2024/04/12/area-of-circle.html" rel="alternate" type="text/html" title="How Humanity Discovered the Area of a Circle" /><published>2024-04-12T00:00:00-04:00</published><updated>2024-04-12T00:00:00-04:00</updated><id>http://localhost:4000/mathematics/2024/04/12/area-of-circle</id><content type="html" xml:base="http://localhost:4000/mathematics/2024/04/12/area-of-circle.html"><![CDATA[<!-- Summary:  -->

<!-- PELICAN_END_SUMMARY -->

<p>I was recently thinking about the widely-known equation for the area of a circle, \(A=\pi r^2\). I realized that even though I have taught this equation to dozens of students, I did not know how the equation is derived. In other words, yes this is the correct equation. But <em>why</em> is it? Furthermore, how did humanity arrive at this equation anciently? In my search for an intuitive answer, I found myself liking an approach using regular figures the best. This is more or less the approach that Archimedes (287-212 BC) used to prove \(A= \pi r^2\).</p>

<h3 id="archimedes-method">Archimedes’ Method</h3>
<p>Before I dive into the regular shapes that Archimedes used, I will start simple: with the area of a triangle. It makes sense that the area of a triangle is \(A=\frac{1}{2}bh\) because a triangle is a rectangle cut in half. You can draw a rectangle and call one side “b” for base and the other “h” for height (\(A=bh\)). Cut it in half and it will produce two triangles. So, since cutting a rectangle in half makes a triangle, we know that the area of a triangle is \(A=\frac{1}{2}bh\).</p>

<p><img src="/images/square.png" alt="square" title="A triangle is a square cut in half." /></p>

<p>This helps me prove another well-established formula, the formula for the area of a regular shape. A <em>regular</em> shape is one that has all congruent/equal sides and angles. Here is a regular hexagon. You can see that it is made up of six equal sides, which also gives us six equal triangles.</p>

<p><img src="/images/hexagon.png" alt="hexagon" title="Finding the area of a regular hexagon" /></p>

<p>Let’s look at the bottom triangle. Just like any triangle, we have a base, “b”, and a height, “h”. For any one of the triangles, the area is \(A=\frac{1}{2} bh\). You could just multiply this by six identical triangles to get the area of the whole hexagon, or \(A=6*\frac{1}{2} bh\). It makes sense that the area for the entire shape is just based on the number of sides or triangles in that shape. We can replace the “6” in this formula with a placeholder “n” to denote the number of sides. This allows us to step past hexagons to a regular shape with any number of sides.</p>

<p>\(A=n*\frac{1}{2} bh\) </p>

<p>\(n\) is the number of sides in a regular shape. \(\frac{1}{2}bh\) is the area of each triangle in the shape.</p>

<p>We can see that \(n*b\), or the number of sides times the length of one side, just equals the <em>perimeter</em>, the distance around the shape. We can substitute \(nb=p\) to drop the number of sides and just focus on the distance around the shape.</p>

<p>Using the the formula above (rearranged),</p>

<p>\(A=\frac{1}{2} h*nb\) </p>

<p>we get</p>

<p>\(A=\frac{1}{2} hp\) </p>

<p>where \(p\) is the perimeter and \(h\) is the height of one of the triangles.</p>

<p>Next, in geometry, this “\(h\)” from the diagram is called an <em>apothem</em>, or the line from the center to the side of a regular shape. So instead of “\(h\)” I will call it \(a\). This gives us a standard equation I like to teach.</p>

<p>The area of a regular shape:
\(A=\frac{1}{2}ap\): \(a\) is for apothem and \(p\) is for perimeter.</p>

<p>What happens if we increase the number of sides to infinity? This shape would  become a circle. We call the <em>perimeter</em> of this shape the <em>circumference</em> (C), and the <em>apothem</em> would become equivalent to a <em>radius</em> (r). Here, at 16 sides (below), the shape already begins to look like a circle.</p>

<p><img src="/images/circle.png" alt="circle" title="A circle is a regular shape with infinite sides." /></p>

<p>We just studied what happens when the number of sides of a regular figure approaches infinity: it becomes a circle. We will rename the variables from the last formula for more circle-friendly variables.</p>

<p>From the formula above, \(A=\frac{1}{2}ap\), we get:
\(A=\frac{1}{2} rC\), where C is circumference and r is radius.</p>

<p>The formula for circumference is \(C=\pi d\), or pi times the diameter. This formula is, in fact, how \(\pi\) is defined: it is the ratio between the circumference and the diameter of a circle. We usually write this formula as \(C=2\pi r\) because the diameter is double the radius. Let’s put this into our formula.</p>

<p>From above, we have:</p>

<p>\(A=\frac{1}{2} Cr\) </p>

<p>Substitute \(2 \pi r=C\):</p>

<p>\(A=\frac{1}{2} * 2 \pi r* r\) </p>

<p>Therefore,</p>

<p>\(A=\pi r^2\) </p>

<h3 id="conclusion">Conclusion</h3>
<p>Here we have Archimedes’ method for deriving the area of a circle. Centuries later, others have pondered this same question and have given us additional methods to prove \(A=\pi r^2\). In modern times, calculus can also be used to derive this equation.</p>

<p>As we look at the formulas we learn in school from a historical perspective, we can appreciate how the modern world of mathematics came to be.</p>

<p>For more information, visit <a href="https://en.wikipedia.org/wiki/Area_of_a_circle">“Area of a Circle” on Wikipedia</a>.
Figures were produced using <a href="https://www.math10.com/en/geometry/geogebra/fullscreen.html">GeoGebra</a>.</p>]]></content><author><name>Kevin Price</name></author><category term="Mathematics" /><category term="Mathematics" /><summary type="html"><![CDATA[I was recently thinking about the widely-known equation for the area of a circle, \(A=\pi r^2\). I realized that even though I have taught this equation to dozens of students, I did not know how the equation is derived. In other words, yes this is the correct equation. But why is it? Furthermore, how did humanity arrive at this equation anciently? In my search for an intuitive answer, I found myself liking an approach using regular figures the best. This is more or less the approach that Archimedes (287-212 BC) used to prove \(A= \pi r^2\). Archimedes’ Method Before I dive into the regular shapes that Archimedes used, I will start simple: with the area of a triangle. It makes sense that the area of a triangle is \(A=\frac{1}{2}bh\) because a triangle is a rectangle cut in half. You can draw a rectangle and call one side “b” for base and the other “h” for height (\(A=bh\)). Cut it in half and it will produce two triangles. So, since cutting a rectangle in half makes a triangle, we know that the area of a triangle is \(A=\frac{1}{2}bh\). This helps me prove another well-established formula, the formula for the area of a regular shape. A regular shape is one that has all congruent/equal sides and angles. Here is a regular hexagon. You can see that it is made up of six equal sides, which also gives us six equal triangles. Let’s look at the bottom triangle. Just like any triangle, we have a base, “b”, and a height, “h”. For any one of the triangles, the area is \(A=\frac{1}{2} bh\). You could just multiply this by six identical triangles to get the area of the whole hexagon, or \(A=6*\frac{1}{2} bh\). It makes sense that the area for the entire shape is just based on the number of sides or triangles in that shape. We can replace the “6” in this formula with a placeholder “n” to denote the number of sides. This allows us to step past hexagons to a regular shape with any number of sides. \(A=n*\frac{1}{2} bh\)  \(n\) is the number of sides in a regular shape. \(\frac{1}{2}bh\) is the area of each triangle in the shape. We can see that \(n*b\), or the number of sides times the length of one side, just equals the perimeter, the distance around the shape. We can substitute \(nb=p\) to drop the number of sides and just focus on the distance around the shape. Using the the formula above (rearranged), \(A=\frac{1}{2} h*nb\)  we get \(A=\frac{1}{2} hp\)  where \(p\) is the perimeter and \(h\) is the height of one of the triangles. Next, in geometry, this “\(h\)” from the diagram is called an apothem, or the line from the center to the side of a regular shape. So instead of “\(h\)” I will call it \(a\). This gives us a standard equation I like to teach. The area of a regular shape: \(A=\frac{1}{2}ap\): \(a\) is for apothem and \(p\) is for perimeter. What happens if we increase the number of sides to infinity? This shape would become a circle. We call the perimeter of this shape the circumference (C), and the apothem would become equivalent to a radius (r). Here, at 16 sides (below), the shape already begins to look like a circle. We just studied what happens when the number of sides of a regular figure approaches infinity: it becomes a circle. We will rename the variables from the last formula for more circle-friendly variables. From the formula above, \(A=\frac{1}{2}ap\), we get: \(A=\frac{1}{2} rC\), where C is circumference and r is radius. The formula for circumference is \(C=\pi d\), or pi times the diameter. This formula is, in fact, how \(\pi\) is defined: it is the ratio between the circumference and the diameter of a circle. We usually write this formula as \(C=2\pi r\) because the diameter is double the radius. Let’s put this into our formula. From above, we have: \(A=\frac{1}{2} Cr\)  Substitute \(2 \pi r=C\): \(A=\frac{1}{2} * 2 \pi r* r\)  Therefore, \(A=\pi r^2\)  Conclusion Here we have Archimedes’ method for deriving the area of a circle. Centuries later, others have pondered this same question and have given us additional methods to prove \(A=\pi r^2\). In modern times, calculus can also be used to derive this equation. As we look at the formulas we learn in school from a historical perspective, we can appreciate how the modern world of mathematics came to be. For more information, visit “Area of a Circle” on Wikipedia. Figures were produced using GeoGebra.]]></summary></entry><entry><title type="html">Comparing the US States</title><link href="http://localhost:4000/data%20analysis/2019/07/12/state-map.html" rel="alternate" type="text/html" title="Comparing the US States" /><published>2019-07-12T00:00:00-04:00</published><updated>2019-07-12T00:00:00-04:00</updated><id>http://localhost:4000/data%20analysis/2019/07/12/state-map</id><content type="html" xml:base="http://localhost:4000/data%20analysis/2019/07/12/state-map.html"><![CDATA[<p>I have created a new web page to <a href="https://us-states-comparison.netlify.app">compare the US states</a> to help you decide where to live. The site uses light/dark gradients (choropleth) to graph data directly on a map of the states. The idea for this site came from a conversation with a friend.</p>]]></content><author><name>Kevin Price</name></author><category term="Data Analysis" /><category term="Data Analysis" /><category term="JavaScript" /><summary type="html"><![CDATA[I have created a new web page to compare the US states to help you decide where to live. The site uses light/dark gradients (choropleth) to graph data directly on a map of the states. The idea for this site came from a conversation with a friend.]]></summary></entry><entry><title type="html">Erin Price’s Author Website</title><link href="http://localhost:4000/web%20design/2017/11/19/erin-website.html" rel="alternate" type="text/html" title="Erin Price’s Author Website" /><published>2017-11-19T00:00:00-05:00</published><updated>2017-11-19T00:00:00-05:00</updated><id>http://localhost:4000/web%20design/2017/11/19/erin-website</id><content type="html" xml:base="http://localhost:4000/web%20design/2017/11/19/erin-website.html"><![CDATA[<p>My wife Erin asked me to build her an author website; this can be found at <a href="erinpricewrites.netlify.app">erinpricewrites.netlify.app</a>. The site introduces you to her short stories and books. I invite you to look at her page and appreciate the treasures that are her written works.</p>]]></content><author><name>Kevin Price</name></author><category term="Web Design" /><category term="JavaScript" /><summary type="html"><![CDATA[My wife Erin asked me to build her an author website; this can be found at erinpricewrites.netlify.app. The site introduces you to her short stories and books. I invite you to look at her page and appreciate the treasures that are her written works.]]></summary></entry><entry><title type="html">An Artificial Gravity Computer Model</title><link href="http://localhost:4000/web%20design/2017/08/31/gravity-model.html" rel="alternate" type="text/html" title="An Artificial Gravity Computer Model" /><published>2017-08-31T00:00:00-04:00</published><updated>2017-08-31T00:00:00-04:00</updated><id>http://localhost:4000/web%20design/2017/08/31/gravity-model</id><content type="html" xml:base="http://localhost:4000/web%20design/2017/08/31/gravity-model.html"><![CDATA[<!-- Summary:  -->

<p>If you were to toss a coin on an artificial gravity space station, where would it end up? If you try my new <a href="https://coriolis-station.netlify.app">computer model</a>, you will discover that it won’t land at your feet where you would expect it to. I created this React web app to demonstrate the Coriolis effect, which would be apparent in artificial gravity, or gravity produced by spinning. This model teaches a complicated Physics principle in a simple way.</p>

<p>I am very pleased with the result, the product of many months of work. This page is both a computer model and an article. The following text describes some of the steps I took to generate this model.</p>

<!-- PELICAN_END_SUMMARY -->

<p>###Coding goals</p>

<p>As I state in <a href="https://coriolis-station-article.netlify.app">my article</a>, the Coriolis effect is an effect of rotating environments (artificial gravity) that distorts objects’ movement, causing objects to move to where you do not expect them. My goal was to take this complicated physics principle and simplify it so that anyone can understand it. I wanted to also <em>quantify</em> how “weird” each throw is with a simple number.</p>

<p>I wanted the model to be powerful enough to be able to render whatever movement a user wishes to model. With this objective, I hoped to make the effects of artificial gravity perfectly clear. Furthermore, I wanted the page to be compelling, clean, intuitive, understandable, beautiful, fast, resizable, mobile-friendly, and not glitchy.</p>

<p>###Steps to produce the model</p>

<p>This model came about in several stages. The page started from a simple idea, though the final result is more sophisticated than what I had initially planned.</p>

<ol>
  <li>
    <p>The first stage of this model was to calculate the landing position of the outer space coin toss, so I could quantify how “weird” the toss is. The model needed to <em>compute</em> these coordinates for the viewer. <a href="https://coriolis-station-article.netlify.app/math/">This page</a> explains the math I used.</p>
  </li>
  <li>
    <p>With the landing coordinates, I could calculate the distance the coin would land away from the person; this is the measurable result of the Coriolis effect. I wrote the math <a href="images/CoriolisTable.py">into Python code</a> to generate a graph that shows the relationship between the diameter of the station and the “weirdness” of the throw produced by the Coriolis effect. These graphs assume the coin was tossed from four feet up to a total height of 7 feet.</p>
  </li>
  <li>
    <p>I wanted to demonstrate what any throw would <em>look</em> like. I generated a series of points following the coin’s movement, allowing me to plot these points on an HTML canvas element. These points needed to be in the <em>thrower’s</em> reference frame. Again, here is <a href="https://coriolis-station-article.netlify.app/math-geometry/">the math</a> I used to develop this portion of the project.</p>
  </li>
  <li>
    <p>I wrote an article about the Coriolis effect. I wanted this article to be complete, understandable, informative, and entertaining.</p>
  </li>
  <li>
    <p>I improved the model’s user interface to make it clean and professional. For instance, I added intuitive logarithmic sliders so that it would be easy to change the throw’s parameters.</p>
  </li>
</ol>

<p>###Some things I encountered while coding
Here are some insights into the challenges I encountered while coding this webpage.</p>

<p>#####Insufficient Decimal Places</p>

<p>I discovered that an ordinary variable in Python or JavaScript, with fifteen decimal places, is not accurate enough to calculate the results that I wanted. These results were glitchy! I found a solution with the Decimal.js JavaScript library, which makes it possible to use longer decimals in Javascript. In Python, I made use of the Decimal package.</p>

<p>Here is my glitchy Python graph with the standard number of decimal places:</p>

<p><img src="/img/CoriolisGraphGlitchy.png" alt="glitchy_graph" title="The Python graph is glitchy with the standard number of decimal places." /></p>

<p>Now, with additional decimal places:</p>

<p><img src="/img/CoriolisGraph.png" alt="smooth_graph" title="The Python graph is smooth with 30 decimal places." /></p>

<p>The Python code I wrote to generate these graphs can be found <a href="/img/CoriolisTable.py">here</a>.</p>

<p>#####A Hidden Feature: Querystrings</p>

<p>Just in case I want to demonstrate something about the Coriolis effect, I added the ability to <em>open</em> the model page with specific settings. Querystrings are extra modifiers at the end of your URL; it might look like this: http://coriolis-station.netlify.app/#&amp;diameter=400.</p>

<p>Here are some of the possible querystrings one could use:</p>

<ul>
  <li><em>diameter</em> (&amp;diameter=400): diameter of the station</li>
  <li><em>radius</em> (&amp;radius=5000): radius of the station</li>
  <li><em>percenttime</em> (&amp;percenttime=50): show the movement as slower or faster than reality</li>
  <li><em>startheight</em> (&amp;startheight=30): the height the movement starts from</li>
  <li><em>gravity</em> (&amp;gravity=50): percent of Earth’s gravity on the floor</li>
  <li><em>speed</em>   (&amp;speed=20): speed of the object</li>
  <li><em>angle</em> (&amp;angle=20): angle of the throw in degrees</li>
  <li><em>units</em> (&amp;units=m): either ft, feet, imperial, metric, meters, or m</li>
  <li><em>thrownUp</em> (&amp;thrownup=8): a hidden feature. Instead of specifying the angle and speed, you can simply tell it how high you would have thrown the object on Earth.</li>
  <li><em>statsMenu</em> (&amp;statsmenu=true): I can open the model with the statistics menu open. Possible values: either just “&amp;statsMenu” or “&amp;statsMenu=true”.</li>
  <li><em>inputMenu</em> (&amp;inputmenu=false): I can open the model with the input menu open or closed. Note, this menu is already open by default in most views.</li>
</ul>

<p>#####Logarithmic Sliders</p>

<p>I produced logarithmic sliders for the model’s input. This way, it is intuitive what ranges are acceptable. I found the logic I needed at <a href="https://codepen.io/willat600series/pen/ojzYJx">this site</a>.</p>

<p>My logarithmic ranges needed to <em>include</em> zero. The log of zero does not normally compute! My code uses an offset to allow me to use zero while ensuring the computer does not attempt to calculate the log of zero.</p>

<p>###Conclusion</p>

<p>This was a very fun project that kept me involved for several years. I was happy to find some simple solutions to my complex coding problems. I hope I taught you something about artificial gravity!</p>]]></content><author><name>Kevin Price</name></author><category term="Web Design" /><category term="Computer Modeling" /><category term="React" /><category term="JavaScript" /><summary type="html"><![CDATA[If you were to toss a coin on an artificial gravity space station, where would it end up? If you try my new computer model, you will discover that it won’t land at your feet where you would expect it to. I created this React web app to demonstrate the Coriolis effect, which would be apparent in artificial gravity, or gravity produced by spinning. This model teaches a complicated Physics principle in a simple way. I am very pleased with the result, the product of many months of work. This page is both a computer model and an article. The following text describes some of the steps I took to generate this model. ###Coding goals As I state in my article, the Coriolis effect is an effect of rotating environments (artificial gravity) that distorts objects’ movement, causing objects to move to where you do not expect them. My goal was to take this complicated physics principle and simplify it so that anyone can understand it. I wanted to also quantify how “weird” each throw is with a simple number. I wanted the model to be powerful enough to be able to render whatever movement a user wishes to model. With this objective, I hoped to make the effects of artificial gravity perfectly clear. Furthermore, I wanted the page to be compelling, clean, intuitive, understandable, beautiful, fast, resizable, mobile-friendly, and not glitchy. ###Steps to produce the model This model came about in several stages. The page started from a simple idea, though the final result is more sophisticated than what I had initially planned. The first stage of this model was to calculate the landing position of the outer space coin toss, so I could quantify how “weird” the toss is. The model needed to compute these coordinates for the viewer. This page explains the math I used. With the landing coordinates, I could calculate the distance the coin would land away from the person; this is the measurable result of the Coriolis effect. I wrote the math into Python code to generate a graph that shows the relationship between the diameter of the station and the “weirdness” of the throw produced by the Coriolis effect. These graphs assume the coin was tossed from four feet up to a total height of 7 feet. I wanted to demonstrate what any throw would look like. I generated a series of points following the coin’s movement, allowing me to plot these points on an HTML canvas element. These points needed to be in the thrower’s reference frame. Again, here is the math I used to develop this portion of the project. I wrote an article about the Coriolis effect. I wanted this article to be complete, understandable, informative, and entertaining. I improved the model’s user interface to make it clean and professional. For instance, I added intuitive logarithmic sliders so that it would be easy to change the throw’s parameters. ###Some things I encountered while coding Here are some insights into the challenges I encountered while coding this webpage. #####Insufficient Decimal Places I discovered that an ordinary variable in Python or JavaScript, with fifteen decimal places, is not accurate enough to calculate the results that I wanted. These results were glitchy! I found a solution with the Decimal.js JavaScript library, which makes it possible to use longer decimals in Javascript. In Python, I made use of the Decimal package. Here is my glitchy Python graph with the standard number of decimal places: Now, with additional decimal places: The Python code I wrote to generate these graphs can be found here. #####A Hidden Feature: Querystrings Just in case I want to demonstrate something about the Coriolis effect, I added the ability to open the model page with specific settings. Querystrings are extra modifiers at the end of your URL; it might look like this: http://coriolis-station.netlify.app/#&amp;diameter=400. Here are some of the possible querystrings one could use: diameter (&amp;diameter=400): diameter of the station radius (&amp;radius=5000): radius of the station percenttime (&amp;percenttime=50): show the movement as slower or faster than reality startheight (&amp;startheight=30): the height the movement starts from gravity (&amp;gravity=50): percent of Earth’s gravity on the floor speed (&amp;speed=20): speed of the object angle (&amp;angle=20): angle of the throw in degrees units (&amp;units=m): either ft, feet, imperial, metric, meters, or m thrownUp (&amp;thrownup=8): a hidden feature. Instead of specifying the angle and speed, you can simply tell it how high you would have thrown the object on Earth. statsMenu (&amp;statsmenu=true): I can open the model with the statistics menu open. Possible values: either just “&amp;statsMenu” or “&amp;statsMenu=true”. inputMenu (&amp;inputmenu=false): I can open the model with the input menu open or closed. Note, this menu is already open by default in most views. #####Logarithmic Sliders I produced logarithmic sliders for the model’s input. This way, it is intuitive what ranges are acceptable. I found the logic I needed at this site. My logarithmic ranges needed to include zero. The log of zero does not normally compute! My code uses an offset to allow me to use zero while ensuring the computer does not attempt to calculate the log of zero. ###Conclusion This was a very fun project that kept me involved for several years. I was happy to find some simple solutions to my complex coding problems. I hope I taught you something about artificial gravity!]]></summary></entry><entry><title type="html">How Computers Think: A Resume Job Field Classification Model in Python</title><link href="http://localhost:4000/machine%20learning/2017/08/20/resume-model.html" rel="alternate" type="text/html" title="How Computers Think: A Resume Job Field Classification Model in Python" /><published>2017-08-20T00:00:00-04:00</published><updated>2017-08-20T00:00:00-04:00</updated><id>http://localhost:4000/machine%20learning/2017/08/20/resume-model</id><content type="html" xml:base="http://localhost:4000/machine%20learning/2017/08/20/resume-model.html"><![CDATA[<!-- Summary: A Python model to classify resumes based on their job type. -->

<p>Can a computer correctly classify your resume into its correct job field? I recently built a classification model for my work, Mosaic Data Science, that answers this question. This model is about 85% accurate in my tests.</p>

<p>My boss has granted me permission to post an article about this project to my website. I will use this project to teach you how a computer “thinks.” I hope that this article may enlighten you about how machine learning works.</p>

<!-- PELICAN_END_SUMMARY -->

<p><img style="float: right; margin-left:10px; margin-top:-5px;" src="/images/fixing-unplugged-computer-clipart_750-754.png" width="275" height="275" alt="https://clipartfest.com/download/bb6ad6c6ec516c0883d3f33d885b02cca169f21b.html" /></p>

<p>This project was written as a demonstration for DARPA, the Defense Advanced Research Projects Agency. DARPA has another project in mind for Mosaic, contingent on our demonstration of this resume classification model; thus I was tasked with building it. I am grateful for the valuable data science advice I continued to receive from the employees around me as I built this model.</p>

<h3 id="the-model">The Model</h3>
<p>After a great deal of experimenting with the advice of my colleagues, I discovered a <a href="http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html">code sample on scikit-learn’s website</a> that does what I am looking for. This code classifies news articles into 20 unique newsgroups, which is a very similar problem to classifying resumes by job-field.</p>

<p>After experimenting with many other model versions, I ultimately adopted this site’s code and the score improved dramatically.</p>

<p>I experimented with the available classifiers (model types) from scikit-learn, and I determined that the best model type for this feature setup was a Random Forest Classifier.</p>

<p>A random forest uses a collection of what are called decision trees. A decision tree is a small piece of a model, which collectively takes the resumes and make a web of statistical choices about each resume. It would attempt to find which words are most strongly correlated with each topic and then use this information to make a category decision.</p>

<p>To explain decision trees, I will use an example from the Kaggle website. I spent some time studying their dataset from the Titanic disaster. Kaggle is hosting a machine-learning competition to generate a model that can predict the survival of Titanic passengers. For this model, the most obvious decision tree feature would be whether the passenger is male or female; females had a much higher survivability rate than males. Next, I could gauge which class the passenger was in. Third class had poor survivability, while first class was much more likely to escape alive. These features would be different nodes on a decision tree. There is also an element of randomness in the decision tree; thus, it will not necessarily produce exactly the same result every time.</p>

<p>For the resume classification project, a decision tree would map out correlations between words. The computer generates a forest of different decision trees and averages out their answers to get the best category choice for each resume. This is a random forest.</p>

<h3 id="data-source">Data Source</h3>

<p><img style="float: left; margin-right:10px;" src="/images/k19403833.png" width="200" height="200" alt="Resume clip-art" />
 <!--obtained from http://www.fotosearch.com/CSP538/k19403833/ (now defunct)--></p>

<p>In order to build a resume classifier, I need resumes to classify! Fortunately, another intern at the company was tasked with the job of downloading batches of resumes from Indeed.com using shell scripts which were written by my supervisor. This intern downloaded 1000 resumes for each of 100 different job fields, in batches of 50. His job must have been very tedious.</p>

<p>As I combed through the resume data, my boss and I discovered a problem: the downloaded resumes did not always match the supposed job field they represented! As I was showing my boss the input files, he found a resume that appeared completely irrelevant. This surprised me, and prompted me to investigate further. My colleagues and I had previously assumed this data was “correct.” In other words, we thought what we were downloading actually represented the field it was supposed to represent. Unfortunately, my boss and I only discovered this problem after the temporary employee who downloaded the resumes had already left. A computer model will not work if it is trained on faulty data!</p>

<p>At this point, the model already appeared to be working marvellously! The model produced a high score, but obviously the underlying code was producing faulty predictions, since the data was incorrect. Something must be similar in these resumes, or the model would not be able to classify the resumes at all. The computer model was successfully finding trends in the faulty resume data.</p>

<p>The downloads appeared to work like a Google search, with the most relevant search results at the top of the list, and less relevant results later. As you went down the results list, searches for “aircraft mechanic” might start turning up auto mechanics, for example. Then, completely irrelevant results would start turning up. Something in these resumes must have matched; the model was successfully classifying them.</p>

<p>This problem was more obvious with some fields than with others; for instance, there were almost no “helicopter pilot” resumes available at all, but there were hundreds of dental hygienists. This trend probably depends on which job-fields are most searched for on Indeed.com.</p>

<p>To fix this problem, I created an algorithm to verify the resume results. It matched each resume “headline”—a metadata field that was not displayed on the actual resume—to the job field it was supposed to represent. If any of the words in the job field were not in the headline, then that resume was thrown out. Furthermore, if my purging strategy left me with less than 100 resumes, then I threw out that job field altogether, dismissing it as low-quality. This appeared to fix the problem. It did not significantly change the model’s score, but the returned resumes finally appeared to match the fields they <em>should</em> represent.</p>

<h3 id="natural-language-processing">Natural Language Processing</h3>
<p>Building this computer model requires language processing. The computer must be able, at some level, to understand the text that it is being fed.</p>

<p>The computer needed to process, simplify, and standardize the text in each resume. It needed to mathematically rate how important each word is to its particular job-field. It had to generate a standard list of features that the model could look at and compare.</p>

<p>Unfortunately, computers are not advanced enough to read text and “understand” what this text means. Companies have worked at this problem for years and have generated some very clever scripts. Some algorithms are able to extract a good deal of meaning from text, but no computer is able to simply “read” a random document and explain to you “in its own words” what it means. There are some highly complex algorithms out there to extract limited meaning from text, but such approaches are much more expensive and less common. A human can use English grammar to read a resume; a computer looks at text through statistics and mathematics.</p>

<p>Natural Language Processing is an emerging field that uses computers to extract meaning from text. Most of the work in this field uses a “bag of words” theory to study documents. In other words, the computer looks at a document statistically as if it were simply a pile of words, and it attempts to figure out which words are relevant to the topic.</p>

<p><img style="float: right; margin-left:10px; margin-bottom:10px;" src="/images/1252997748713475.jpeg" width="350" height="350" alt="http://www.okclipart.com/Scrabble-Players-Clip-Art30khensaze/" /></p>

<p>In order for my “bag of words” to be useful to a computer, I needed to standardize the text of each resume. I threw out all extraneous characters such as bullets and punctuation, and kept only the characters that follow the ASCII format. In other words, I kept only standard characters and eliminated any foreign characters or symbols. I removed all capitalization. I lemmatized everything: I simplified each word to the word root. Lemmas allow the model to recognize many forms of a word as one word. Type, typing, and typed each share the same lemma (or root) of “type.” Three separate words would only confuse the model.</p>

<p>Next, we used mathematics to determine which words are important to each resume. The process we used is called TF-IDF, or Term Frequency-Inverse Document Frequency. Fortunately, scikit-learn provides a tool to do this automatically. I will explain.</p>

<p>A word that is important to a document should occur frequently in that document and infrequently in the others. TF-IDF capitalizes on this by counting up all of the words in each of the resumes and comparing these counts to each other. Each word in every resume gets a TF-IDF “score” which tells the computer how important the word theoretically is to that resume. All of these TF-IDF scores were placed in a massive table and compared to each other. This table contains all of the possible words in the entire dataset as columns (millions of unique words), and all of the different resumes as rows. If a word does not exist in a resume, then it simply gets a score of 0.</p>

<p>TF-IDF rates how important each word is to a particular resume, but it does not rate how important the word is to the job field! To accomplish this, I discovered another handy feature in the scikit-learn package: a chi^2 importance test. Essentially, the script uses this test to determine which words in my large TF-IDF table were most correlated to each job field. I used this tool to determine which resume words to keep and which to discard. This left me with a small overall subset of highly relevant words. I determined that the overall top 1000 most-relevant words was the best selection.</p>

<p>If you would like, here is a snippet of code from the text processing function (article continues below):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def process(fields, allkeys, docID, map_output, res_output, jobwords):
	"""Takes each resume in .json format and processes it"""

	#remove duplicate keys before processing them. I want unique resumes.
	key = fields['accountKey'] #get the document key
	if key in allkeys:
		return
	
	#Now do a quality check, to make sure the resume actually is what we think it is.
	if not check_jobwords( fields, jobwords ):
		return
		
	string = extract(fields)    #Extract resume fields into a string
	string = remove_garbage(string)   #Remove garbage characters
		
	#skip resume if it's less than 10 words. Don't want it.
	if len(string.split()) &lt;= 10:
		return
	
	string = lemmatize(string) #lemmatize
	
	#Write the results to the disk
	res_output.write( str(docID) + " " + string + '\n' ) #"Processed_resumes.txt"
	map_output.write( str(docID) + ',' + key + '\n') #docID : document-key mapping
	
	return key   #this is necessary to prevent duplicate resumes.
</code></pre></div></div>

<h3 id="principal-component-analysis">Principal Component Analysis</h3>

<p>One of the requirements of the project was that it include Principal Component Analysis. PCA is a method that can reduce the number of features used by the model while losing as little data as possible.</p>

<p>Here is an example of how PCA works. Say a group of points are correlated and produce a nearly straight line. You could simplify the data by flattening it out, and still capture the majority of the variation in the data. Essentially, you can “boil down” the data to something simpler, while still capturing most of what makes the data unique. This process is called “dimensionality reduction.”</p>

<p><img style="float: center; margin-bottom:10px;" src="/images/PCA.png" width="600" height="200" alt="PCA demonstration" title="http://setosa.io/ev/principal-component-analysis/" /></p>

<p>What the random forest model “sees” is a huge table, with each possible word making a new column, and each resume making a row. The different columns in my data table are called “features.” After processing them through SVD, they become “components” (hence principal <em>component</em> analysis). My table was 1000 columns (words) by ~80,000 rows (resumes) large. Within the table, you see the TF-IDF scores from each word; that is what the table saves.</p>

<p>Theoretically, less data input would mean less “overfitting.” In other words, reducing the data would streamline the model and prevent it from extrapolating excessive information or seeing trends where there are none. Unfortunately, using PCA brought the score down slightly instead of improving it, hence the model is 84% accurate instead of 91%.</p>

<p>For my model, I actually used a method that is very similar to PCA called Singular Value Decomposition (SVD), because the data input is sparse. The general process used is the same, but SVD can work with sparse inputs.</p>

<p>Using sparse data allows the computer to use less computer memory to store information. If most of the datapoints are zero, then instead of saving all of the “zeros,” we can save only the datapoints that actually have data. This is the case for most of the words in my table, since most of the possible words will not be represented in a given resume.</p>

<p><img style="float: right; margin-left:10px; margin-bottom:10px;" src="/images/ExplainedVariance.png" width="415" height="415" alt="graph of variance from each new component" /></p>

<p>I graphed the amount of information gained by each new SVD component, and I determined that 200 was about the optimal number of components (see the graph). After 200 components (boiled-down words), the model does not gain “new” information as quickly from each new component.</p>

<p>I used PCA to reduce 1000 word-features into 200! To review, PCA finds correlations among the words and finds the simplest way to represent this data. So, the first 200 PCA components will show the majority of variation from the 1000 words. Thus, I can minimize the data stream that is fed into the model by incorporating fewer features.</p>

<h3 id="other-tests">Other Tests</h3>

<p>I experimented with many other features as I refined the model, with the continued help of my colleagues. I tried using different packages for the features I needed. For instance, I tried using Gensim’s TF-IDF algorithm instead of scikit-learn’s. I discovered that scikit-learn’s algorithm is much faster and more effective for computer modeling.</p>

<p>I tested the different model types found in the 20-newsgroups sample on scikit-learn’s website (I introduced this above); the Random Forest model scored the best.</p>

<p>Another version of my random forest model used bigrams instead of raw resume words. In other words, I matched all of the words in pairs of two in an attempt to capture connections between words. I then used TF-IDF to capture only those bigrams that contain words that are in the top 10% of relevance. This model was dramatically worse; it produced a much lower score.</p>

<h3 id="scoring">Scoring</h3>
<p>There are several different metrics to use to score a classification model. Each score provides a different window into how the model is performing. I used accuracy, precision, recall, and fscore. The computer randomly selects a segment of the resumes to <em>train</em> the model and another segment to <em>test</em> its predictive power and produce a score.</p>

<p>Accuracy simply looks at the model and says, “Of all of the answers, how many did I get right?” Precision asks, “Of all of the answers that were labeled in category X, how many actually are in category X?” And then recall asks, “Of all of the answers that should be in category X, how many did you correctly mark in category X?” The fscore attempts to strike a balance between precision and recall.</p>

<p>Because of my resume validation exercise (purging invalid job fields), the number of resumes in each field was dramatically different. Some fields contained 100 resumes; some fields contained 800! This confused the model and produced a convoluted set of scores:</p>

<table border="1" cellspacing="8" cellpadding="8" style="font-family:Georgia; margin-left:10px;">
<tr>
    <td>Accuracy</td>
    <td>86.3%</td>
</tr>
<tr>
    <td>Precision</td>
    <td>84.2%</td>
</tr>
<tr>
    <td>Recall</td>
    <td>74.0%</td>
</tr>
<tr>
    <td>Fscore</td>
    <td>76.4%</td>
</tr>
</table>
<p><br /></p>

<p>To balance these scores, I discovered a class_weight=”balanced” feature hidden in the Random Forest code. This tells the model that the different job field categories are unevenly stacked. Simply adding this setting improved all of the scores:</p>

<table border="1" cellspacing="8" cellpadding="8" style="font-family:Georgia; margin-left:10px;">
<tr>
    <td>Accuracy</td>
    <td>89.2%</td>
</tr>
<tr>
    <td>Precision</td>
    <td>88.9%</td>
</tr>
<tr>
    <td>Recall</td>
    <td>85.4%</td>
</tr>
<tr>
    <td>Fscore</td>
    <td>86.3%</td>
</tr>
</table>
<p><br /></p>

<p>Adding in the SVD (PCA) reduced the score slightly and resulted in this output:</p>

<p><img style="float: center; margin-bottom:10px;" src="/images/output.png" alt="final output" /></p>

<p>A few additional changes to the model brought the score up to 84%.</p>

<h3 id="conclusion">Conclusion</h3>
<p>The computer can successfully identify a job field from a resume. This is especially surprising considering that many people change job fields; thus, their resumes may contain aspects of both fields.</p>

<p>In a model with ~90 job fields and thousands of resumes, an 84% accuracy score seems very impressive. If the number of included job fields were increased dramatically, it is likely that the model would become confused between similar fields, such as “aircraft mechanic” and “auto mechanic.” However, at the current level of complexity, the model functions very well.</p>]]></content><author><name>Kevin Price</name></author><category term="Machine Learning" /><category term="Machine Learning" /><category term="Python" /><summary type="html"><![CDATA[Can a computer correctly classify your resume into its correct job field? I recently built a classification model for my work, Mosaic Data Science, that answers this question. This model is about 85% accurate in my tests. My boss has granted me permission to post an article about this project to my website. I will use this project to teach you how a computer “thinks.” I hope that this article may enlighten you about how machine learning works. This project was written as a demonstration for DARPA, the Defense Advanced Research Projects Agency. DARPA has another project in mind for Mosaic, contingent on our demonstration of this resume classification model; thus I was tasked with building it. I am grateful for the valuable data science advice I continued to receive from the employees around me as I built this model. The Model After a great deal of experimenting with the advice of my colleagues, I discovered a code sample on scikit-learn’s website that does what I am looking for. This code classifies news articles into 20 unique newsgroups, which is a very similar problem to classifying resumes by job-field. After experimenting with many other model versions, I ultimately adopted this site’s code and the score improved dramatically. I experimented with the available classifiers (model types) from scikit-learn, and I determined that the best model type for this feature setup was a Random Forest Classifier. A random forest uses a collection of what are called decision trees. A decision tree is a small piece of a model, which collectively takes the resumes and make a web of statistical choices about each resume. It would attempt to find which words are most strongly correlated with each topic and then use this information to make a category decision. To explain decision trees, I will use an example from the Kaggle website. I spent some time studying their dataset from the Titanic disaster. Kaggle is hosting a machine-learning competition to generate a model that can predict the survival of Titanic passengers. For this model, the most obvious decision tree feature would be whether the passenger is male or female; females had a much higher survivability rate than males. Next, I could gauge which class the passenger was in. Third class had poor survivability, while first class was much more likely to escape alive. These features would be different nodes on a decision tree. There is also an element of randomness in the decision tree; thus, it will not necessarily produce exactly the same result every time. For the resume classification project, a decision tree would map out correlations between words. The computer generates a forest of different decision trees and averages out their answers to get the best category choice for each resume. This is a random forest. Data Source In order to build a resume classifier, I need resumes to classify! Fortunately, another intern at the company was tasked with the job of downloading batches of resumes from Indeed.com using shell scripts which were written by my supervisor. This intern downloaded 1000 resumes for each of 100 different job fields, in batches of 50. His job must have been very tedious. As I combed through the resume data, my boss and I discovered a problem: the downloaded resumes did not always match the supposed job field they represented! As I was showing my boss the input files, he found a resume that appeared completely irrelevant. This surprised me, and prompted me to investigate further. My colleagues and I had previously assumed this data was “correct.” In other words, we thought what we were downloading actually represented the field it was supposed to represent. Unfortunately, my boss and I only discovered this problem after the temporary employee who downloaded the resumes had already left. A computer model will not work if it is trained on faulty data! At this point, the model already appeared to be working marvellously! The model produced a high score, but obviously the underlying code was producing faulty predictions, since the data was incorrect. Something must be similar in these resumes, or the model would not be able to classify the resumes at all. The computer model was successfully finding trends in the faulty resume data. The downloads appeared to work like a Google search, with the most relevant search results at the top of the list, and less relevant results later. As you went down the results list, searches for “aircraft mechanic” might start turning up auto mechanics, for example. Then, completely irrelevant results would start turning up. Something in these resumes must have matched; the model was successfully classifying them. This problem was more obvious with some fields than with others; for instance, there were almost no “helicopter pilot” resumes available at all, but there were hundreds of dental hygienists. This trend probably depends on which job-fields are most searched for on Indeed.com. To fix this problem, I created an algorithm to verify the resume results. It matched each resume “headline”—a metadata field that was not displayed on the actual resume—to the job field it was supposed to represent. If any of the words in the job field were not in the headline, then that resume was thrown out. Furthermore, if my purging strategy left me with less than 100 resumes, then I threw out that job field altogether, dismissing it as low-quality. This appeared to fix the problem. It did not significantly change the model’s score, but the returned resumes finally appeared to match the fields they should represent. Natural Language Processing Building this computer model requires language processing. The computer must be able, at some level, to understand the text that it is being fed. The computer needed to process, simplify, and standardize the text in each resume. It needed to mathematically rate how important each word is to its particular job-field. It had to generate a standard list of features that the model could look at and compare. Unfortunately, computers are not advanced enough to read text and “understand” what this text means. Companies have worked at this problem for years and have generated some very clever scripts. Some algorithms are able to extract a good deal of meaning from text, but no computer is able to simply “read” a random document and explain to you “in its own words” what it means. There are some highly complex algorithms out there to extract limited meaning from text, but such approaches are much more expensive and less common. A human can use English grammar to read a resume; a computer looks at text through statistics and mathematics. Natural Language Processing is an emerging field that uses computers to extract meaning from text. Most of the work in this field uses a “bag of words” theory to study documents. In other words, the computer looks at a document statistically as if it were simply a pile of words, and it attempts to figure out which words are relevant to the topic. In order for my “bag of words” to be useful to a computer, I needed to standardize the text of each resume. I threw out all extraneous characters such as bullets and punctuation, and kept only the characters that follow the ASCII format. In other words, I kept only standard characters and eliminated any foreign characters or symbols. I removed all capitalization. I lemmatized everything: I simplified each word to the word root. Lemmas allow the model to recognize many forms of a word as one word. Type, typing, and typed each share the same lemma (or root) of “type.” Three separate words would only confuse the model. Next, we used mathematics to determine which words are important to each resume. The process we used is called TF-IDF, or Term Frequency-Inverse Document Frequency. Fortunately, scikit-learn provides a tool to do this automatically. I will explain. A word that is important to a document should occur frequently in that document and infrequently in the others. TF-IDF capitalizes on this by counting up all of the words in each of the resumes and comparing these counts to each other. Each word in every resume gets a TF-IDF “score” which tells the computer how important the word theoretically is to that resume. All of these TF-IDF scores were placed in a massive table and compared to each other. This table contains all of the possible words in the entire dataset as columns (millions of unique words), and all of the different resumes as rows. If a word does not exist in a resume, then it simply gets a score of 0. TF-IDF rates how important each word is to a particular resume, but it does not rate how important the word is to the job field! To accomplish this, I discovered another handy feature in the scikit-learn package: a chi^2 importance test. Essentially, the script uses this test to determine which words in my large TF-IDF table were most correlated to each job field. I used this tool to determine which resume words to keep and which to discard. This left me with a small overall subset of highly relevant words. I determined that the overall top 1000 most-relevant words was the best selection. If you would like, here is a snippet of code from the text processing function (article continues below): def process(fields, allkeys, docID, map_output, res_output, jobwords): """Takes each resume in .json format and processes it""" #remove duplicate keys before processing them. I want unique resumes. key = fields['accountKey'] #get the document key if key in allkeys: return #Now do a quality check, to make sure the resume actually is what we think it is. if not check_jobwords( fields, jobwords ): return string = extract(fields) #Extract resume fields into a string string = remove_garbage(string) #Remove garbage characters #skip resume if it's less than 10 words. Don't want it. if len(string.split()) &lt;= 10: return string = lemmatize(string) #lemmatize #Write the results to the disk res_output.write( str(docID) + " " + string + '\n' ) #"Processed_resumes.txt" map_output.write( str(docID) + ',' + key + '\n') #docID : document-key mapping return key #this is necessary to prevent duplicate resumes. Principal Component Analysis One of the requirements of the project was that it include Principal Component Analysis. PCA is a method that can reduce the number of features used by the model while losing as little data as possible. Here is an example of how PCA works. Say a group of points are correlated and produce a nearly straight line. You could simplify the data by flattening it out, and still capture the majority of the variation in the data. Essentially, you can “boil down” the data to something simpler, while still capturing most of what makes the data unique. This process is called “dimensionality reduction.” What the random forest model “sees” is a huge table, with each possible word making a new column, and each resume making a row. The different columns in my data table are called “features.” After processing them through SVD, they become “components” (hence principal component analysis). My table was 1000 columns (words) by ~80,000 rows (resumes) large. Within the table, you see the TF-IDF scores from each word; that is what the table saves. Theoretically, less data input would mean less “overfitting.” In other words, reducing the data would streamline the model and prevent it from extrapolating excessive information or seeing trends where there are none. Unfortunately, using PCA brought the score down slightly instead of improving it, hence the model is 84% accurate instead of 91%. For my model, I actually used a method that is very similar to PCA called Singular Value Decomposition (SVD), because the data input is sparse. The general process used is the same, but SVD can work with sparse inputs. Using sparse data allows the computer to use less computer memory to store information. If most of the datapoints are zero, then instead of saving all of the “zeros,” we can save only the datapoints that actually have data. This is the case for most of the words in my table, since most of the possible words will not be represented in a given resume. I graphed the amount of information gained by each new SVD component, and I determined that 200 was about the optimal number of components (see the graph). After 200 components (boiled-down words), the model does not gain “new” information as quickly from each new component. I used PCA to reduce 1000 word-features into 200! To review, PCA finds correlations among the words and finds the simplest way to represent this data. So, the first 200 PCA components will show the majority of variation from the 1000 words. Thus, I can minimize the data stream that is fed into the model by incorporating fewer features. Other Tests I experimented with many other features as I refined the model, with the continued help of my colleagues. I tried using different packages for the features I needed. For instance, I tried using Gensim’s TF-IDF algorithm instead of scikit-learn’s. I discovered that scikit-learn’s algorithm is much faster and more effective for computer modeling. I tested the different model types found in the 20-newsgroups sample on scikit-learn’s website (I introduced this above); the Random Forest model scored the best. Another version of my random forest model used bigrams instead of raw resume words. In other words, I matched all of the words in pairs of two in an attempt to capture connections between words. I then used TF-IDF to capture only those bigrams that contain words that are in the top 10% of relevance. This model was dramatically worse; it produced a much lower score. Scoring There are several different metrics to use to score a classification model. Each score provides a different window into how the model is performing. I used accuracy, precision, recall, and fscore. The computer randomly selects a segment of the resumes to train the model and another segment to test its predictive power and produce a score. Accuracy simply looks at the model and says, “Of all of the answers, how many did I get right?” Precision asks, “Of all of the answers that were labeled in category X, how many actually are in category X?” And then recall asks, “Of all of the answers that should be in category X, how many did you correctly mark in category X?” The fscore attempts to strike a balance between precision and recall. Because of my resume validation exercise (purging invalid job fields), the number of resumes in each field was dramatically different. Some fields contained 100 resumes; some fields contained 800! This confused the model and produced a convoluted set of scores: Accuracy 86.3% Precision 84.2% Recall 74.0% Fscore 76.4% To balance these scores, I discovered a class_weight=”balanced” feature hidden in the Random Forest code. This tells the model that the different job field categories are unevenly stacked. Simply adding this setting improved all of the scores: Accuracy 89.2% Precision 88.9% Recall 85.4% Fscore 86.3% Adding in the SVD (PCA) reduced the score slightly and resulted in this output: A few additional changes to the model brought the score up to 84%. Conclusion The computer can successfully identify a job field from a resume. This is especially surprising considering that many people change job fields; thus, their resumes may contain aspects of both fields. In a model with ~90 job fields and thousands of resumes, an 84% accuracy score seems very impressive. If the number of included job fields were increased dramatically, it is likely that the model would become confused between similar fields, such as “aircraft mechanic” and “auto mechanic.” However, at the current level of complexity, the model functions very well.]]></summary></entry><entry><title type="html">About This Site</title><link href="http://localhost:4000/about%20this%20site/2017/02/09/about-this-site.html" rel="alternate" type="text/html" title="About This Site" /><published>2017-02-09T00:00:00-05:00</published><updated>2017-02-09T00:00:00-05:00</updated><id>http://localhost:4000/about%20this%20site/2017/02/09/about-this-site</id><content type="html" xml:base="http://localhost:4000/about%20this%20site/2017/02/09/about-this-site.html"><![CDATA[<p>Welcome to my website! I built this site using a site generator in Python called Pelican. I found a fantastic Pelican theme (Lannisport) and heavily modified it to make it my own.</p>

<p>I will post my coding projects here. To see my code feel free to browse <a href="https://www.github.com/kevindprice">my GitHub page</a>.</p>]]></content><author><name>Kevin Price</name></author><category term="About this site" /><category term="Pelican" /><category term="coding" /><summary type="html"><![CDATA[Welcome to my website! I built this site using a site generator in Python called Pelican. I found a fantastic Pelican theme (Lannisport) and heavily modified it to make it my own. I will post my coding projects here. To see my code feel free to browse my GitHub page.]]></summary></entry></feed>